{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05cd823a",
   "metadata": {},
   "source": [
    "In this notebook, we explore active learning when already having a set of experimentally determined TCR-APL library pairs. The question is: which APLs should we measure for a new TCR, to be able to predict the remaining ones with highest performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cedd3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a96af1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from tqdm import tqdm\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f958153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from preprocessing import add_activation_thresholds, full_aa_features, get_aa_features \n",
    "from preprocessing import get_complete_dataset\n",
    "                \n",
    "from utils_al import train_classification_model, predict_classification_on_test, train_regression_model\n",
    "from utils_al import predict_regression_on_test, evaluate_classification_models, evaluate_regression_models\n",
    "from utils_al import get_aa_blosum, get_metrics_cls, get_metrics_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0588bc",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d714ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE_EPITOPE = 'VPSVWRSSL'\n",
    "BASE_EPITOPE = 'SIINFEKL'\n",
    "\n",
    "if BASE_EPITOPE == 'SIINFEKL':\n",
    "    NORMALIZATION = 'AS'\n",
    "    THRESHOLD = '46.9'\n",
    "else:\n",
    "    NORMALIZATION = 'pc'\n",
    "    THRESHOLD = '66.09'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19084e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = get_complete_dataset()\n",
    "    data = data[data['mut_pos']>=0]\n",
    "    data = add_activation_thresholds(data)\n",
    "    \n",
    "    data['full_sample'] = data[['epitope', 'tcr']].apply(tuple, axis=1)\n",
    "    data = data[data['normalization'] == NORMALIZATION]\n",
    "    data = data[data['threshold'] == THRESHOLD]\n",
    "    data = data[data['is_educated'] == True]\n",
    "    aa_features = get_aa_features()\n",
    "    features = full_aa_features(data, aa_features[['factors']], include_tcr=True)\n",
    "    return data, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a8cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tumor_data():\n",
    "    data = get_complete_dataset(BASE_EPITOPE)\n",
    "    data = data[data['mut_pos']>=0]\n",
    "    data = add_activation_thresholds(data, epitope=BASE_EPITOPE)\n",
    "    data['full_sample'] = data[['epitope', 'tcr']].apply(tuple, axis=1)\n",
    "    data = data[data['normalization'] == NORMALIZATION]\n",
    "    data = data[data['threshold'] == THRESHOLD]\n",
    "    aa_features = get_aa_features()\n",
    "    features = full_aa_features(data, aa_features[['factors']], base_peptide=BASE_EPITOPE, include_tcr=True)\n",
    "    return data, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39bffb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BASE_EPITOPE == 'SIINFEKL':\n",
    "    data, features = load_data()\n",
    "else:\n",
    "    data, features = load_tumor_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baf5c26",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55f2e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_model(samples_train, seed):\n",
    "    mask = data['full_sample'].isin(samples_train)\n",
    "    data_train = data[mask]\n",
    "    feat_train = features[mask]\n",
    "    y_train = data_train['is_activated']\n",
    "    clf = RandomForestClassifier(n_estimators=1000, random_state=seed).fit(feat_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fdef5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classification_on_test(clf, samples_train):\n",
    "    mask = ~data['full_sample'].isin(samples_train)\n",
    "    data_test = data[mask]\n",
    "    feat_test = features[mask]\n",
    "    \n",
    "    y_test = data_test['is_activated']\n",
    "    p_test = clf.predict_proba(feat_test)\n",
    "    p_test = p_test[:, (1 if p_test.shape[1] == 2 else 0)]\n",
    "    return p_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1bcc474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression_model(samples_train, seed):\n",
    "    mask = data['full_sample'].isin(samples_train)\n",
    "    data_train = data[mask]\n",
    "    feat_train = features[mask]\n",
    "    y_train = data_train['activation']\n",
    "    reg = RandomForestRegressor(n_estimators=250, max_features='sqrt', criterion='mae', random_state=seed).fit(feat_train, y_train)\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0116690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_regression_on_test(reg, samples_train):\n",
    "    mask = ~data['full_sample'].isin(samples_train)\n",
    "    data_test = data[mask]\n",
    "    feat_test = features[mask]\n",
    "    \n",
    "    y_test = data_test['activation']\n",
    "    y_pred = reg.predict(feat_test)\n",
    "    return y_pred, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b447371",
   "metadata": {},
   "source": [
    "## Aquisition Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a011d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_random(samples_train, clf, reg, N):\n",
    "    remaining_samples = data[~data['full_sample'].isin(samples_train)]\n",
    "    remaining_samples = remaining_samples['full_sample']\n",
    "    remaining_samples = remaining_samples.values\n",
    "    new_samples = np.random.choice(remaining_samples, N, replace=False).tolist()\n",
    "    return new_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19771f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_active(samples_train, clf, reg, N):\n",
    "    p_test, _ = predict_classification_on_test(clf, samples_train)\n",
    "    \n",
    "    threshold = 0.5 # sum(p_test) / len(p_test)\n",
    "    uncertainties = np.abs(p_test-threshold)\n",
    "    idx_top_uncertain = uncertainties.argsort()[:N]\n",
    "    data_test = data[~data['full_sample'].isin(samples_train)]\n",
    "    new_samples = data_test.iloc[idx_top_uncertain]['full_sample'].tolist()\n",
    "    return new_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b6c3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_blosum_set():\n",
    "    list_apls = []\n",
    "    blosum_mat = get_aa_blosum()\n",
    "    for idx, aa_base in enumerate(BASE_EPITOPE):\n",
    "        new_aa_idx = blosum_mat.loc[aa_base].argmin()\n",
    "\n",
    "        new_aa = blosum_mat.columns[new_aa_idx]\n",
    "        new_epitope = BASE_EPITOPE[:idx] + new_aa + BASE_EPITOPE[idx+1:]\n",
    "        list_apls.append(new_epitope)\n",
    "    return list_apls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a1660",
   "metadata": {},
   "source": [
    "## Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6852aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_learning_loop(tcr, method_data_aquisition, \n",
    "                      N=8, M=5, seed=0):\n",
    "    samples_train = data[data['tcr']!=tcr]['full_sample'].values.tolist()\n",
    "    \n",
    "    results = {}\n",
    "    for metric_name in list(metrics_class[0].keys()) + list(metrics_class[1].keys()) + list(metrics_reg.keys()):\n",
    "        results[metric_name] = []\n",
    "    \n",
    "    clf = train_classification_model(samples_train, seed)\n",
    "    reg = train_regression_model(samples_train, seed)\n",
    "    \n",
    "    samples_tcr = []\n",
    "    for idx in range(M):\n",
    "        new_samples = method_data_aquisition(samples_train, clf, reg, N)\n",
    "        samples_train += new_samples\n",
    "        samples_tcr += [el[0] for el in new_samples]\n",
    "        \n",
    "        clf = train_classification_model(samples_train, seed)\n",
    "        p_test, c_truth = predict_classification_on_test(clf, samples_train)\n",
    "        evaluate_classification_models(c_truth, p_test, metrics_class, results, idx)\n",
    "        \n",
    "        reg = train_regression_model(samples_train, seed)\n",
    "        y_test, y_truth = predict_regression_on_test(reg, samples_train)\n",
    "        evaluate_regression_models(y_truth, y_test, metrics_reg, results, idx)\n",
    "    \n",
    "    path_apls = f'../results/active_learning/across/{BASE_EPITOPE}_selectedAPLs_'\n",
    "    path_apls += f'{method_data_aquisition.__name__}.csv'\n",
    "    if not os.path.exists(path_apls):\n",
    "        with open(path_apls, 'w') as f:\n",
    "            f.write('seed,tcr,apls\\n')\n",
    "\n",
    "    with open(path_apls, 'a') as f:\n",
    "        f.write(f'{seed},{tcr},{str(samples_tcr)}\\n')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fb83498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(method_data_aquisition, N, M, do_reload=False):\n",
    "    global data\n",
    "    global features\n",
    "    results = {}\n",
    "    for metric_name in list(metrics_reg.keys()) + list(metrics_class[0].keys()) + list(metrics_class[1].keys()):\n",
    "        results[metric_name] = []\n",
    "\n",
    "    for i in tqdm(range(10)):\n",
    "        for tcr in data['tcr'].unique():\n",
    "            scores = run_learning_loop(tcr, method_data_aquisition, N=N, M=M, seed=i)\n",
    "            for name, score in scores.items():\n",
    "                results[name] += [[tcr] + val for val in score]\n",
    "            if do_reload:\n",
    "                data, features = load_data()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86c58ea",
   "metadata": {},
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4dc2aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_plots(summary, palette, x_ticks, x_labels, plot_type):\n",
    "    sns.set(font_scale=2)\n",
    "    sns.set_style('white')\n",
    "\n",
    "    for name in list(list(metrics_class[0].keys()) + list(metrics_class[1].keys()) + list(metrics_reg.keys())):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        dfs_results = []\n",
    "        for method in summary.keys():\n",
    "            df = pd.DataFrame(summary[method][name])\n",
    "            df.columns = ['tcr', 'iteration', name]\n",
    "            df['method'] = method\n",
    "            dfs_results.append(df)\n",
    "        df_joint = pd.concat(dfs_results)\n",
    "        plot = plot_type(data=df_joint, x='iteration', y=name, hue='method', palette=palette)\n",
    "              \n",
    "        \n",
    "        sns.despine(bottom=False, left=False)\n",
    "\n",
    "        plot.set(xlabel='Amount Samples')\n",
    "        if len(name)<=3:\n",
    "            plot.set(ylabel=name.upper())\n",
    "\n",
    "        plt.legend(title='Sampling Method')\n",
    "        plot.set_xticks(x_ticks)\n",
    "        plot.set_xticklabels(x_labels)\n",
    "        plot.figure.savefig(f'../figures/active_learning/active_across_class_{name}_N-{N}.png', dpi=300)\n",
    "        plot.figure.savefig(f'../figures/active_learning/active_across_class_{name}_N-{N}.pdf', dpi=300)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5085e8",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "432534a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_run_experiment(name, aquisition_method, do_reload=False):\n",
    "    path_out = f'../results/active_learning/across/{BASE_EPITOPE}_crossTCR_FULL_{name}_{N}_{M}.json'\n",
    "    if not os.path.exists(path_out) or REDO:\n",
    "        results_exp = run_experiment(aquisition_method, N, M, do_reload)\n",
    "        with open(path_out, 'w') as json_file:\n",
    "            json.dump(results_exp, json_file)\n",
    "    else:\n",
    "        with open(path_out) as f:\n",
    "            results_exp = json.load(f)\n",
    "    return results_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "566f6bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_reg = get_metrics_reg()\n",
    "metrics_class = get_metrics_cls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f8c1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n",
    "M = 10\n",
    "REDO = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84910447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "results_random = save_run_experiment('random', add_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d9ae4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_active = save_run_experiment('active', add_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e80f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_greedy_bound():\n",
    "    path_in = f'../results/active_learning/across/greedy/{BASE_EPITOPE}/'\n",
    "    res_files = [path_in + f for f in os.listdir(path_in) if os.path.isfile(os.path.join(path_in, f))]\n",
    "\n",
    "    results_upper = {}\n",
    "    for name in list(metrics_reg.keys()) + list(metrics_class[0].keys()) + list(metrics_class[1].keys()):\n",
    "        results_upper[name] = []\n",
    "\n",
    "    for path_file in res_files:\n",
    "        with open(path_file) as f:\n",
    "            res_tmp = json.load(f)\n",
    "        for mtc, vals in res_tmp.items():\n",
    "            results_upper[mtc] += vals\n",
    "    return results_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7aa7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_full = {\n",
    "    'active': results_active,\n",
    "    'random': results_random,\n",
    "    #'upper': read_greedy_bound()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd2da8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "palette = {'active': 'tab:red', 'upper': 'tab:olive', 'random': 'tab:cyan'}\n",
    "x_ticks = list(range(0, M))\n",
    "x_labels = ['9'] + [str(9+(i+1)*N) for i in range(M-1)]\n",
    "\n",
    "do_plots(summary_full, palette, x_ticks, x_labels, sns.lineplot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mutagenesis]",
   "language": "python",
   "name": "conda-env-mutagenesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
